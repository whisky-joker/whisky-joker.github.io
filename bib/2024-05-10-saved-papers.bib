@article{daf19f6e6564f7de43dca62ca67a8f9b9f69d067,
title = {Real-time large-scale dense RGB-D SLAM with volumetric fusion},
year = {2014},
url = {https://www.semanticscholar.org/paper/daf19f6e6564f7de43dca62ca67a8f9b9f69d067},
abstract = {We present a new simultaneous localization and mapping (SLAM) system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an ‘as-rigid-as-possible’ space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
author = {Thomas Whelan and M. Kaess and H. Johannsson and M. Fallon and J. Leonard and J. McDonald},
journal = {The International Journal of Robotics Research},
volume = {34},
pages = {598 - 626},
doi = {10.1177/0278364914551008},
}

@article{0389da0dc8aacb7b1d8ccfb5f13fa01a7afae862,
title = {Real-Time Large-Scale Dense 3D Reconstruction with Loop Closure},
year = {2016},
url = {https://www.semanticscholar.org/paper/0389da0dc8aacb7b1d8ccfb5f13fa01a7afae862},
abstract = {S2 TL;DR: This paper proposes an online framework which delivers a consistent 3D model to the user in real time by splitting the scene into submaps, and adjusting the poses of the submaps as and when required.},
author = {O. Kähler and V. Prisacariu and D. W. Murray},
doi = {10.1007/978-3-319-46484-8_30},
}

@article{158435baa65e5392553c25939b852dd32ab2e010,
title = {RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments},
year = {2012},
url = {https://www.semanticscholar.org/paper/158435baa65e5392553c25939b852dd32ab2e010},
abstract = {RGB-D cameras (such as the Microsoft Kinect) are novel sensing systems that capture RGB images along with per-pixel depth information. In this paper we investigate how such cameras can be used for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. We present RGB-D Mapping, a full 3D mapping system that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment. Visual and depth information are also combined for view-based loop-closure detection, followed by pose optimization to achieve globally consistent maps. We evaluate RGB-D Mapping on two large indoor environments, and show that it effectively combines the visual and shape information available from RGB-D cameras.},
author = {Peter Henry and Michael Krainin and E. Herbst and Xiaofeng Ren and D. Fox},
journal = {The International Journal of Robotics Research},
volume = {31},
pages = {647 - 663},
doi = {10.1177/0278364911434148},
}

@article{985adfd574afb86ead62250b394972a637f7f849,
title = {Real-Time Reconstruction of Static and Dynamic Scenes},
year = {2015},
url = {https://www.semanticscholar.org/paper/985adfd574afb86ead62250b394972a637f7f849},
abstract = {With the release of the Microso Xbox 360 Kinect, an affordable real-time RGB-D sensor is now available on the mass market. is makes new techniques and algorithms, which have previously been only available to researchers and enthusiasts, accessible for an everyday use by a broad audience. Applications range from the acquisition of detailed high-quality reconstructions of everyday objects to tracking the complex motions of people. In addition, the captured data can be directly exploited to build virtual reality applications, i.e. virtual mirrors, and can be used for gesture control of devices andmotion analysis. To make these applications easy-to-use in our everyday life, they should be intuitive to control and provide feedback at real-time rates. In this dissertation, we present new techniques and algorithms for building three-dimensional representations of arbitrary objects using only a single commodity RGB-D sensor, manually editing the acquired reconstructions and tracking the non-rigid motion of physically deforming objects at real-time rates. We start by proposing the use of a statistical prior to obtain high-quality reconstructions of the human head using only a single low-quality depth frame of a commodity sensor. We extend this approach and obtain even higher quality reconstructions at realtime rates by exploiting all information of a contiguous RGB-D stream and jointly optimizing for shape, albedo and illumination parameters. ereaer, we show that a moving sensor can be used to obtain super-resolution reconstructions of arbitrary objects at sensor rate by fusing all depth observations. We present strategies that allow us to handle a virtually unlimited reconstruction volume by exploiting a new sparse scene representation in combination with an efficient streaming approach. In addition, we present a handle based deformation paradigm that allows the user to edit the captured geometry, whichmight consist of millions of polygons, using an interactive and intuitive modeling metaphor. Finally, we demonstrate that the motion of arbitrary non-rigidly deforming physical objects can be tracked at real-time rates using a custom high-quality RGB-D sensor.},
author = {M. Zollhöfer},
}

@article{22210625b3e396faa3a35b92251d318e570c5eed,
title = {3D Scanning of High Dynamic Scenes Using an RGB-D Sensor and an IMU on a Mobile Device},
year = {2019},
url = {https://www.semanticscholar.org/paper/22210625b3e396faa3a35b92251d318e570c5eed},
abstract = {With the development of RGB-D sensors and mobile devices, 3D scanning has witnessed great progress in recent years. KinectFusion opens up an era of RGB-D 3D reconstruction, which integrates captured depth images into a voxel-based representation. A number of improvements have been applied to the KinectFusion to reduce large footprint and make the 3D reconstruction on mobile devices possible. However, these methods are designed to handle static scenes. In this paper, we propose a method which can perform the 3D scanning of high dynamic scenes using an RGB-D sensor and an inertial measurement unit (IMU) on a mobile device. We first introduce a novel method to segment the depth images into static and dynamic elements with the use of sparse optical flow and the rotational part of the IMU. Then, we determine the camera pose with pixels labeled as static. At last depth, images are integrated into a voxel-based representation. The truncated signed distance function values of static voxels are updated while dynamic voxels are set to free-space. The experiments show that compared with some state-of-the-art systems, our method has better results when scanning high dynamic scenes and has comparable results when scanning low dynamic and static scenes. Besides, our method processes eight frames/s on an Apple iPad Air 2.},
author = {Yangdong Liu and Wei Gao and Zhanyi Hu},
journal = {IEEE Access},
volume = {7},
pages = {24057-24070},
doi = {10.1109/ACCESS.2019.2900740},
}

@article{e4e2bccb29e13e05e00c66adb317ea9f3b33ba48,
title = {Large-scale, real-time 3D scene reconstruction on a mobile device},
year = {2017},
url = {https://www.semanticscholar.org/paper/e4e2bccb29e13e05e00c66adb317ea9f3b33ba48},
abstract = {S2 TL;DR: This work explores the problem of large-scale, real-time 3D reconstruction on a mobile devices of this type and describes an on-device post-processing method for fusing datasets from multiple, independent trials in order to improve the quality and coverage of the reconstruction.},
author = {Ivan Dryanovski and Matthew Klingensmith and S. Srinivasa and Jizhong Xiao},
journal = {Autonomous Robots},
volume = {41},
pages = {1423 - 1445},
doi = {10.1007/s10514-017-9624-2},
}

@article{d185d0e4064e75096cdf946376fb153d44287300,
title = {Voxblox: Building 3D Signed Distance Fields for Planning},
year = {2016},
url = {https://www.semanticscholar.org/paper/d185d0e4064e75096cdf946376fb153d44287300},
abstract = {Truncated Signed Distance Fields (TSDFs) have become a popular tool in 3D reconstruction, as they allow building very high-resolution models of the environment in real-time on GPU. However, they have rarely been used for planning on robotic platforms, mostly due to high computational and memory requirements. We propose to reduce these requirements by using large voxel sizes, and extend the standard TSDF representation to be faster and better model the environment at these scales. We also propose a method to build Euclidean Signed Distance Fields (ESDFs), which are a common representation for planning, incrementally out of our TSDF representation. ESDFs provide Euclidean distance to the nearest obstacle at any point in the map, and also provide collision gradient information for use with optimization-based planners. We validate the reconstruction accuracy and real-time performance of our combined system on both new and standard datasets from stereo and RGB-D imagery. The complete system will be made available as an open-source library called voxblox.},
author = {Helen Oleynikova and Zachary Taylor and M. Fehr and Juan I. Nieto and R. Siegwart},
journal = {ArXiv},
volume = {abs/1611.03631},
pages = {null},
doi = {10.3929/ETHZ-A-010820353},
}

@article{67ab0fbe4e6587ce759aff1729b9200106ab85b8,
title = {VDBFusion: Flexible and Efficient TSDF Integration of Range Sensor Data},
year = {2022},
url = {https://www.semanticscholar.org/paper/67ab0fbe4e6587ce759aff1729b9200106ab85b8},
abstract = {Mapping is a crucial task in robotics and a fundamental building block of most mobile systems deployed in the real world. Robots use different environment representations depending on their task and sensor setup. This paper showcases a practical approach to volumetric surface reconstruction based on truncated signed distance functions, also called TSDFs. We revisit the basics of this mapping technique and offer an approach for building effective and efficient real-world mapping systems. In contrast to most state-of-the-art SLAM and mapping approaches, we are making no assumptions on the size of the environment nor the employed range sensor. Unlike most other approaches, we introduce an effective system that works in multiple domains using different sensors. To achieve this, we build upon the Academy-Award-winning OpenVDB library used in filmmaking to realize an effective 3D map representation. Based on this, our proposed system is flexible and highly effective and, in the end, capable of integrating point clouds from a 64-beam LiDAR sensor at 20 frames per second using a single-core CPU. Along with this publication comes an easy-to-use C++ and Python library to quickly and efficiently solve volumetric mapping problems with TSDFs.},
author = {Ignacio Vizzo and Tiziano Guadagnino and J. Behley and C. Stachniss},
journal = {Sensors (Basel, Switzerland)},
volume = {22},
pages = {null},
doi = {10.3390/s22031296},
pmid = {35162040},
}

@article{f4093c2270284b4ec9b154c7d06791bf99c7b942,
title = {KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera},
year = {2011},
url = {https://www.semanticscholar.org/paper/f4093c2270284b4ec9b154c7d06791bf99c7b942},
abstract = {KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.},
author = {S. Izadi and David Kim and Otmar Hilliges and D. Molyneaux and Richard A. Newcombe and Pushmeet Kohli and J. Shotton and Steve Hodges and Dustin Freeman and A. Davison and A. Fitzgibbon},
journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
volume = {null},
pages = {null},
doi = {10.1145/2047196.2047270},
}

@article{c2adb138674a7f75ade139d0e56d7a8e83f0803e,
title = {Real-time 3D reconstruction at scale using voxel hashing},
year = {2013},
url = {https://www.semanticscholar.org/paper/c2adb138674a7f75ade139d0e56d7a8e83f0803e},
abstract = {Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.},
author = {M. Nießner and M. Zollhöfer and S. Izadi and M. Stamminger},
journal = {ACM Transactions on Graphics (TOG)},
volume = {32},
pages = {1 - 11},
doi = {10.1145/2508363.2508374},
}

@article{e2975deb809f35bd47ac3de34e348675a0cfb3f6,
title = {KinectFusion: Real-time dense surface mapping and tracking},
year = {2011},
url = {https://www.semanticscholar.org/paper/e2975deb809f35bd47ac3de34e348675a0cfb3f6},
abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
author = {Richard A. Newcombe and S. Izadi and Otmar Hilliges and D. Molyneaux and David Kim and A. Davison and Pushmeet Kohli and J. Shotton and Steve Hodges and A. Fitzgibbon},
journal = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
volume = {null},
pages = {127-136},
doi = {10.1109/ISMAR.2011.6092378},
}

@article{bae5d50b59282140648224274af73931f2c58711,
title = {Voxblox: Incremental 3D Euclidean Signed Distance Fields for on-board MAV planning},
year = {2016},
url = {https://www.semanticscholar.org/paper/bae5d50b59282140648224274af73931f2c58711},
abstract = {Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and flexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulfill these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs). We propose a method to incrementally build ESDFs from Truncated Signed Distance Fields (TSDFs), a common implicit surface representation used in computer graphics and vision. TSDFs are fast to build and smooth out sensor noise over many observations, and are designed to produce surface meshes. We show that we can build TSDFs faster than Octomaps, and that it is more accurate to build ESDFs out of TSDFs than occupancy maps. Our complete system, called voxblox, is available as open source and runs in real-time on a single CPU core. We validate our approach on-board an MAV, by using our system with a trajectory optimization local planner, entirely on-board and in real-time.},
author = {Helen Oleynikova and Zachary Taylor and M. Fehr and R. Siegwart and Juan I. Nieto},
journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
volume = {null},
pages = {1366-1373},
doi = {10.1109/IROS.2017.8202315},
arxivid = {1611.03631},
}

@article{8d39d30f4d7a2df039d43a2984d0d6df71e2f804,
title = {A systematic literature review: Real-time 3D reconstruction method for telepresence system},
year = {2023},
url = {https://www.semanticscholar.org/paper/8d39d30f4d7a2df039d43a2984d0d6df71e2f804},
abstract = {Real-time three-dimensional (3D) reconstruction of real-world environments has many significant applications in various fields, including telepresence technology. When depth sensors, such as those from Microsoft’s Kinect series, are introduced simultaneously and become widely available, a new generation of telepresence systems can be developed by combining a real-time 3D reconstruction method with these new technologies. This combination enables users to engage with a remote person while remaining in their local area, as well as control remote devices while viewing their 3D virtual representation. There are numerous applications in which having a telepresence experience could be beneficial, including remote collaboration and entertainment, as well as education, advertising, and rehabilitation. The purpose of this systematic literature review is to analyze the recent advances in 3D reconstruction methods for telepresence systems and the significant related work in this field. Next, we determine the input data and the technological device employed to acquire the input data, which will be utilized in the 3D reconstruction process. The methods of 3D reconstruction implemented in the telepresence system as well as the evaluation of the system, have been extracted and assessed from the included studies. Through the analysis and summarization of many dimensions, we discussed the input data used for the 3D reconstruction method, the real-time 3D reconstruction methods implemented in the telepresence system, and how to evaluate the system. We conclude that real-time 3D reconstruction methods for telepresence systems have progressively improved over the years in conjunction with the advancement of machines and devices such as Red Green Blue-Depth (RGB-D) cameras and Graphics Processing Unit (GPU).},
author = {F. E. Fadzli and A. W. Ismail and Shafina Abd Karim Ishigaki},
journal = {PLOS ONE},
volume = {18},
pages = {null},
doi = {10.1371/journal.pone.0287155},
pmid = {37967080},
}
